{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data read garyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/anna.txt\",'r') as f:\n",
    "    text=f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char = tuple(set(text))\n",
    "int2char = dict(enumerate(char))\n",
    "char2int = {ch:ii for ii,ch in enumerate(char)}\n",
    "encoded = np.array([char2int[ch] for ch in text])\n",
    "encoded.shape\n",
    "len(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(enc_char,n_seq,n_steps):\n",
    "    #batch_char(number of char in batch) = n_seq(i.e.batch_size)*n_steps(i.e. len of each batch)\n",
    "    batch_char = n_seq * n_steps#say N*M\n",
    "    #e.g. if we have batchsize= 5 and len of each batch = 4 then we will have first input as 5 equal sized batch with value 4\n",
    "    total_no_batch = len(enc_char)// batch_char#// int dinxa\n",
    "    #total char kati ho teslai divide by euta batch ko char garesi ayo\n",
    "    total_char_tokeep = total_no_batch * batch_char\n",
    "    #dim = k*N*M\n",
    "    #thikka batch garauna milne gari char line \n",
    "    enc_char = enc_char[:total_char_tokeep]\n",
    "    \n",
    "    split_char = enc_char.reshape((n_seq,-1))\n",
    "    #split char will be now dim = (N,M*K)\n",
    "    for i in range(0,split_char.shape[1],n_steps): #split_char[1]=M*K i.e. total no of batch(k) and M= no_of_step in each batch\n",
    "        x = split_char[:,i:i+n_steps]\n",
    "        \n",
    "        y = np.zeros_like(x)\n",
    "        \n",
    "        try:\n",
    "            y[:,:-1],y[:,-1] = x[:,1:],x[:,i+n_steps]\n",
    "            \n",
    "        except IndexError:\n",
    "            y[:,:-1],y[:,-1] = x[:,1:],x[:,0]\n",
    "        \n",
    "        yield x,y\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr,n_labels):\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape),n_labels),dtype = np.float32)\n",
    "    #yesle zero matrix banauxa of size (array ko size haru multiplication or flatten can also be done) \n",
    "    #jasari vaheni array lai vector ma convert garera tesko size*n_label ko matrix hunxa\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]),arr.flatten()]=1\n",
    "    #one_hot zero matrix hunxa \n",
    "    \n",
    "    one_hot = one_hot.reshape((*arr.shape,n_labels))\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##batches = get_batches(encoded,10,50)\n",
    "#harek sentence ma 10 ota batch tyo batch 50 step ko\n",
    "##x,y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [[28 48 32 82 41 17  4 49 20 76]\n",
      " [49 32 53 49 37  1 41 49 45  1]\n",
      " [73 30 37 31 76 76 33 75 17 72]\n",
      " [37 49 77  2  4 30 37 45 49 48]\n",
      " [49 30 41 49 30 72 18 49 72 30]\n",
      " [49 26 41 49  6 32 72 76  1 37]\n",
      " [48 17 37 49 36  1 53 17 49  7]\n",
      " [62 49 56  2 41 49 37  1  6 49]\n",
      " [41 49 30 72 37 79 41 31 49 21]\n",
      " [49 72 32 30 77 49 41  1 49 48]]\n",
      "y: [[48 32 82 41 17  4 49 20 76 76]\n",
      " [32 53 49 37  1 41 49 45  1 30]\n",
      " [30 37 31 76 76 33 75 17 72 18]\n",
      " [49 77  2  4 30 37 45 49 48 30]\n",
      " [30 41 49 30 72 18 49 72 30  4]\n",
      " [26 41 49  6 32 72 76  1 37 24]\n",
      " [17 37 49 36  1 53 17 49  7  1]\n",
      " [49 56  2 41 49 37  1  6 49 72]\n",
      " [49 30 72 37 79 41 31 49 21 48]\n",
      " [72 32 30 77 49 41  1 49 48 17]]\n"
     ]
    }
   ],
   "source": [
    "##print(\"x:\",x[:,:10])\n",
    "##print(\"y:\",y[:,:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 83)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.multiply(*x.shape),len(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(500, 83)\n"
     ]
    }
   ],
   "source": [
    "#a=np.zeros((x.flatten().size,len(char)),dtype=np.float32)\n",
    "#print(a)\n",
    "#print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 60 52  2 74 26 23 49 56 55 55 55 14 52  2  2 53 49 16 52  5 43 19 43\n",
      " 26 32 49 52 23 26 49 52 19 19 49 52 19 43 69 26 34 49 26 66 26 23 53 49\n",
      " 54 50 49 52  5 49 50 57 74 49 82 57 43 50 82 49 74 57 49 32 74 52 53 39\n",
      " 61 49 52 50 32 71 26 23 26 17 49 27 50 50 52 39 49 32  5 43 19 43 50 82\n",
      " 39 49 36 54 66 43 50 38 55 55 61 45 26 32 39 49 43 74 80 32 49 32 26 74\n",
      " 74 19 26 17 38 49 44 60 26 49  2 23 43 30 26 49 43 32 49  5 52 82 50 43\n",
      " 16 43 30 26 50 74 50 49 17 54 23 43 50 82 49 60 43 32 49 30 57 50 66 26\n",
      " 23 32 52 74 43 57 50 49 71 43 74 60 49 60 43 32 55 36 23 57 74 60 26 23\n",
      " 49 71 52 32 49 74 60 43 49 43 74 49 43 32 39 49 32 43 23 29 61 49 32 52\n",
      " 43 17 49 74 60 26 49 57 19 17 49  5 52 50 39 49 82 26 74 74 43 50 82 49\n",
      " 54  2 39 49 52 50 17 55 30 23 49 21 74 49 71 52 32 55 57 50 19 53 49 71\n",
      " 60 26 50 49 74 60 26 49 32 52  5 26 49 26 66 26 50 43 50 82 49 60 26 49\n",
      " 30 52  5 26 49 74 57 49 74 60 26 43 60 26 50 49 30 57  5 26 49 16 57 23\n",
      " 49  5 26 39 61 49 32 60 26 49 32 52 43 17 39 49 52 50 17 49 71 26 50 74\n",
      " 49 36 52 30 69 49 43 50 74 57 49 74 60 26 34 49 36 54 74 49 50 57 71 49\n",
      " 32 60 26 49 71 57 54 19 17 49 23 26 52 17 43 19 53 49 60 52 66 26 49 32\n",
      " 52 30 23 43 16 43 30 26 17 39 49 50 57 74 49  5 74 49 43 32 50 80 74 38\n",
      " 49 44 60 26 53 80 23 26 49  2 23 57  2 23 43 26 74 57 23 32 49 57 16 49\n",
      " 52 49 32 57 23 74 39 55 36 54 74 49 71 26 80 23 26 49 49 32 52 43 17 49\n",
      " 74 57 49 60 26 23 32 26 19 16 39 49 52 50 17 49 36 26 82 52 50 49 52 82\n",
      " 52 43 50 49 16 23 57  5 49 74 60 26 49 36 26 82 43 50 50 43]\n"
     ]
    }
   ],
   "source": [
    "#print(x.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##b= np.zeros((2,5),dtype=np.float32)\n",
    "#y= np.array([3,4])\n",
    "#b[np.arange(2),y]=1\n",
    "#b[0,3] ma sure choti 1 ani b[1,4] ma arko choti 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self,tokens,n_hidden=256,num_layer=2,dropout_prob=0.2):\n",
    "        super(CharRNN,self).__init__()\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.n_hidden = n_hidden\n",
    "        self.num_layer = num_layer\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.char2int = {ch:ii for ii, ch in enumerate(self.chars)}\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.chars),n_hidden,num_layer,dropout=dropout_prob,batch_first=True)\n",
    "        #n_hidden = number of feature in hidden state\n",
    "        #num_layer = kati ota Lstm cell stacked up vaera baseko xa\n",
    "        #batch_first = True means i/p and o/p are given\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        \n",
    "        self.linear = nn.Linear(n_hidden,len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self,x,hc):\n",
    "        out,(h,c) = self.lstm(x,hc)\n",
    "        \n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = out.contiguous().view(-1,self.n_hidden)\n",
    "        #contigous is required to stack up multiple LSTM cells as we have 2 LSTM cells defined\n",
    "        \n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out , (h,c)\n",
    "    \n",
    "    def init_weights(self):\n",
    "        #it is done to create initial state of all zeros \n",
    "        initrange = 0.1\n",
    "        \n",
    "        self.linear.bias.data.fill_(0)\n",
    "        #set bias tensor to zero\n",
    "        \n",
    "        self.linear.weight.data.uniform_(-1,1)\n",
    "        \n",
    "    def init_hidden(self,n_seq):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.num_layer, n_seq, self.n_hidden).zero_(),\n",
    "                weight.new(self.num_layer, n_seq, self.n_hidden).zero_())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for param in net.parameters():\n",
    "#    print(param.name,param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At init_hidden :\n",
    "When we create model defining a class with some layers. Each layer has some parameters associated with it. And once we instanciate our network with net=Model(). We can access those parameters as above.\n",
    "\n",
    "Internally these parameters are in iterable mode as parameters() and we can iterate over using next().\n",
    "\n",
    "next(self.parameters()).data will choose one of the parameters of the model. Which one exactly it chooses doesn't matter and it varies randomly depending on where the last pointer location was for the iterable over the parameters() container. Let's say it chooses the parameter fc from the model. When it chooses this, the device information and the data type of fc gets copied to the variable weight along with the data. The data isn't relevant for us, but the device information and the data type are important.\n",
    "\n",
    "new() to create a new variable which is an exact copy of the weight variable and using the inplace function zero_() we get rid of the useless data from the weight variable and set the tensor to zero.\n",
    "\n",
    "So, the basic reason why weight is needed, is so that it can act as a dummy variable that contains information about the device chosen, which we can then utilize to create a new tensor that will be thus automatically on the same device as the rest of the model. Thus essentially it is a trick to write code that will work whether you are training the model on GPU or using CPU.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net,data,epochs=10,n_seq=10,n_steps=50,lr=0.001,clip=5,val_frac=0.1,cuda=False,print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    #to specific training mode such that dropouts and other essential function for training is on\n",
    "    opt = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    #0.1 is fraction then val_idx will be from 0.9 fraction of total len\n",
    "    \n",
    "    dat,val_data = data[:val_idx],data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "        \n",
    "    counter = 0\n",
    "    \n",
    "    n_chars = len(net.chars)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        hidden = net.init_hidden(n_seq)\n",
    "        \n",
    "        for x,y in get_batches(data,n_seq,n_steps):\n",
    "            #x=(10,50) y=(10,50)\n",
    "            counter +=1\n",
    "            x = one_hot_encode(x,n_chars)\n",
    "            #one_hot encode garesi input (batchno.,n_seq,n_char) i.e.(10,50,83)\n",
    "            #now convert into torch\n",
    "            inputs,targets = torch.from_numpy(x),torch.from_numpy(y)\n",
    "            \n",
    "            if cuda:\n",
    "                inputs,targets = inputs.cuda(), targets.cuda()\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h= tuple([each.data for each in hidden])\n",
    "            \n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "            #input = (10,50,83)\n",
    "            # output will be in (n_seqs*n_step , len(chars)) i.e.(500,83)  one encoded o/p\n",
    "            #target is (500) values\n",
    "            loss.backward()\n",
    "            \n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            #for exploding gradient\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = self.init_hidden(n_seq)\n",
    "                val_loss = []\n",
    "                net.eval()\n",
    "                for x,y in get_batches(val_data,n_seq,n_steps):\n",
    "                    x = one_hot_encode(x,n_chars)\n",
    "                    \n",
    "                    x,y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    out , val_h = self.forward(x,val_h)\n",
    "                    \n",
    "                    val_los = criterion(out,y)\n",
    "                    \n",
    "                    val_loss.append(val_loss.item())\n",
    "                 \n",
    "                net.train()    \n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharRNN(\n",
       "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.2)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (linear): Linear(in_features=512, out_features=83, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = CharRNN(char, n_hidden=512, num_layer=2)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter =0\n",
    "hidden = net.init_hidden(10)\n",
    "for x,y in get_batches(encoded,10,50):\n",
    "            #x=(10,50) y=(10,50)\n",
    "    counter +=1\n",
    "    x = one_hot_encode(x,len(net.chars))\n",
    "            #one_hot encode garesi input (batchno.,n_seq,n_char) i.e.(10,50,83)\n",
    "            #now convert into torch\n",
    "    inputs,targets = torch.from_numpy(x),torch.from_numpy(y)\n",
    "    h= tuple([each.data for each in hidden])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50, 83])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       " tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out,hid=net.lstm(inputs,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 50, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 512])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hid[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_fraction' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-c02ea49acc90>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# you may change cuda to True if you plan on using a GPU!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# also, if you do, please INCREASE the epochs to 25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_seq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_seqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-43ca3e7e6371>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data, epochs, n_seq, n_steps, lr, clip, val_frac, cuda, print_every)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mval_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mval_fraction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m#0.1 is fraction then val_idx will be from 0.9 fraction of total len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_fraction' is not defined"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "\n",
    "# you may change cuda to True if you plan on using a GPU!\n",
    "# also, if you do, please INCREASE the epochs to 25\n",
    "train(net, encoded, epochs=1, n_seq=n_seqs, n_steps=n_steps, lr=0.001, cuda=False, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_1_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net,char,h=None,top_k=None):\n",
    "    \n",
    "     # tensor inputs\n",
    "    x = np.array([[net.char2int[char]]])\n",
    "    #encoded input liyo char ko\n",
    "    \n",
    "    x = one_hot_encode(x, len(net.chars))\n",
    "    #one hot encode garyo tesko\n",
    "    \n",
    "    inputs = torch.from_numpy(x)\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "            \n",
    "        inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "    h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "    out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    if(train_on_gpu):\n",
    "        p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(net.chars))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "    p = p.numpy().squeeze()\n",
    "    char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "    return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([[1,2],\n",
    "         [2,4]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "c= []\n",
    "for i in a:\n",
    "    b = i**2\n",
    "    c.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
